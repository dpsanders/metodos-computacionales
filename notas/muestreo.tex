\chapter{Muestreo}
Como vimos en el capítulo anterior, es normalmente imposible llevar a cabo las
sumas requeridas por la teoría de la física estadística para calcular la
función de partición y los promedios deseados. Por lo tanto, es necesario
\defn{muestrear} ciertas configuraciones ``representativas'', es decir, escoger
de manera estadística o aleatoria --pero al mismo tiempo, lista-- las
configuraciones sobre las cuales sumaremos.

La primera idea que podríamos tener es la de muestrear de manera uniforme sobre
todas las configuraciones, escogiéndolas ``al azar'', es decir, con igual
probabilidad, tal que cada espín tiene igual probabilidad de apuntar hacia
arriba o hacia abajo.  Sin embargo, está intuitivamente claro que eso dará
configuraciones $\ss$ que tienen aproximadamente el mismo número de espines para
arriba como para abajo, y por lo tanto, una magnetización $M(\ss)$ y energía
$E(\ss)$ que se concentran alrededor de $0$. Eso sería adecuado para investigar
las propiedades del sistema a altas temperaturas, donde justamente las
configuraciones tienen un peso estadístico, es decir, probabilidad $p(\ss)$, más
o menos uniforme. Sin embargo, a bajas temperaturas, el sistema se concentrará
alrededor de sus estados bases, donde todos los espines se alínean, mientras
que tales configuraciones casi nunca se generarán de manera uniforme.

\section{Muestreo no-uniforme}
Por lo tanto, es necesario introducir un muestreo no-uniforme, es decir,
escoger distintas configuraciones según una distribución de probabilidad
$p(\mu)$ (que no necesariamente es la de Boltzmann). Más adelante veremos cómo
eso se puede hacer; por lo momento, supongamos que ya lo hemos logrado.

Consideremos un muestreo finito de configuraciones generadas según esta
distribución de probabilidad, $(\mu_1, \ldots, \mu_M)$. En una corrida larga,
esperamos que algunas de estas configuraciones son iguales. Enumeremos las
\emph{distintas} configuraciones como $\mu^{(1)}, \ldots, \mu^{(C)}$. Entonces
$\mu^{(i)}$ debería aparecer $M p(\mu^{(i)})$ veces, aproximadamente. 

Si ahora queremos calcular, por ejemplo, la función de partición, pensaríamos
primero en calcular
\begin{equation}
Z \stackrel{\textrm{?}}{\simeq} \sum_{i=1}^M e^{-\beta E(\mu_i)}.
\end{equation}
Sin embargo, eso daría
\begin{equation}
Z \stackrel{\textrm{?}}{\simeq} \sum_{i=1}^C p(\mu^{(i)}) e^{-\beta
E(\mu^{(i)})}.
\end{equation}
Por lo tanto, las configuraciones aparecen en la suma pesada por la
distribución $p(\mu)$ de muestreo que nosotros imponemos.

Para eliminar este efecto no-deseado, es necesario dividir por las $p$. Así
obtenemos las siguientes expresiones aproximadas para las cantidades de interés:
\begin{equation}
 Z \simeq  \sum_{i=1}^M \frac{1}{p(\mu_i)} e^{-\beta
E(\mu_i)};
\end{equation}
\begin{equation}
 \mean{Q} \simeq \frac{\sum_{i=1}^M \textfrac{1}{p(\mu_i)} Q(\mu_i) e^{-\beta
E(\mu_i)}}{\sum_{j=1}^M \textfrac{1}{p(\mu_j)} e^{-\beta
E(\mu_j)}}.
\end{equation}
Estos estimados convergen a los valores verdaderos cuando el número de datos $M$
converge al infinito.
 
\section{Muestreo de Boltzmann}
>Cuál distribución $p(\mu)$ nos dará los mejores resultados? Una posibilidad es la de intentar reproducir el sistema físico de la manera más fiel posible: muestreamos según la distribución de Boltzmann, $p(\mu) = \textfrac{1}{Z} e^{-\beta E(\mu)}$. Al sustituir esta distribución en el estimado, encontramos que
\begin{equation}
 \mean{Q} \simeq \frac{1}{M} \sum_{i=1}^M Q(\mu_i).
\end{equation}
Es decir, el promedio, tomando en cuenta las frecuencias de visita, se reduce a un promedio simple de los datos.

Sin embargo, esto nos lleva a otro problema: <no sabemos cómo generar distintas configuraciones según la distribución de Boltzmann!
La solución, sugerida por primera vez por Metropolis et al.~en los años 1950s, es que podemos generar esta --o cualquier otra-- distribución usando un \defn{proceso estocástico} que \emph{converja} hacia esta distribución. En particular, emplearemos normalmente una \defn{cadena de Markov}.

\section{Cadenas de Markov}
La idea es que generaremos una secuencia de configuraciones $\ss_t$ en el tiempo $t$. En cada paso, partiendo de la configuración $\ss$ actual, el sistema podrá escoger distintas configuraciones $\tt$ en el siguiente paso de tiempo $t+1$. Esta elección no será determinista, sino aleatorio: distintas configuraciones $\tt$ se podrán elegir con distintas \defn{probabilidades de transición} $P(\ss \to \tt)$.  Eso se repite en cada paso de tiempo (discreto), así dando lugar a un proceso estocástico. Para que sea una cadena de Markov, se requiere además que las probabilidades $P(\ss \to \tt)$ dependen solamente del estado actual $\ss$. Una caminata aleatoria normal es un ejemplo.

Denotemos por $w_\ss(t)$ la probabilidad de que el sistema se encuentre en el estado $\ss$ al tiempo $t$. Entonces tenemos que
\begin{equation}
 w_\tt(t+1) = \sum_{\ss} w_\ss(t) P(\ss \to \tt).
\end{equation}
Eso es, la probabilidad de estar en cierto estado en el siguiente paso de tiempo es igual a la suma de las probabilidades de que estaba en otra configuración en el paso anterior, por la probabilidad de que la nueva configuración se escogió.
Aunque la suma es, en principio, sobre todas las configuraciones del sistema, en la práctica se reduce a una suma sobre solamente las configuraciones de donde se puede alcanzar la configuración nueva $\tt$. 

Al examinar la forma del miembro de derecho en esta ecuación, nos percatamos de que tiene una forma parecida a una multiplicación de matrices. Por lo tanto, se puede reescribir como
\begin{equation}
 \ww(t+1) = \PP \cdot \ww(t).
\end{equation}
Nótese que tenemos que definir las componentes de la matriz $\PP$ como $P_{\tt \ss} \defeq P(\ss \to \tt)$ para que salga esta ecuación.

Al iterar esta relación, tenemos que
\begin{equation}
 \ww(t) = \PP^t \cdot \ww(0).
\end{equation}
Por lo tanto, el comportamiento asintótico en el tiempo de las probabilidades $w_\mu(t)$ depende de los valores y vectores propios de la matriz $\PP$ de probabilidades de transición.  Por ejemplo, si el valor propio más grande fuera menor que $1$ en módulo, entonces todas las probabilidades se decaerían a $0$. Sin embargo, dado que $\sum_\mu w_\mu(t) = 1$ para todo $t$, esto no es posible.
De hecho, esta condición de normalización implica que el valor propio de mayor módulo es $1$, y los demás valores propios son menores que $1$ en módulo.  Por lo tanto, la distribución converge a un límite $\ww(\infty)$ cuando $t \to \infty$, que satisface una invarianza bajo la acción de $\PP$:
\begin{equation}
w_\tt(\infty) = \sum_\ss w_\ss(\infty) P(\ss \to \tt)
\label{eq:eqm-distn-w}
\end{equation}
Es decir, esta distribución ya no cambia en el tiempo. Por lo tanto, la podemos considerar como una distribución de \defn{equilibrio}.
De hecho, es justamente la distribución --única-- de equilibrio $p(\ss)$ que genera esta cadena de Markov.

Distintas cadenas de Markov difieren en sus probabilidades de transición $P(\ss \to \tt)$. Al cambiar estas probabilidades de transición, se cambia en general la distribución de equilibrio que se genera. Para poder generar una distribución de equilibrio dada, es crucial, por lo tanto, averiguar cómo se relacionan la $\PP$ y la $p(\ss)$ que generar.


\section{Condiciones de balance detallado y ergodicidad}

Reescribiendo la ecuación \eqref{eq:eqm-distn-w} en términos de la distribución de equilibrio $p(\ss)$, obtenemos
\begin{equation}
 p(\tau) = \sum_\ss p(\ss) P(\ss \to \tau).
\end{equation}
Pero también podemos escribir el miembro de izquierda como
\begin{equation}
  p(\tau) = p(\tau) \sum_\ss P(\tau \to \ss) = \sum_\ss p(\tau) P(\tau \to \ss),
\end{equation}
donde la primera igualdad utiliza la normalización que la suma vale $1$ (``hay que ir a algún lado'').
Finalmente, llegamos a la \defn{condición de balance} 
\begin{equation}
 \sum_\ss p(\tau) P(\tau \to \ss) = \sum_\ss p(\ss) P(\ss \to \tau).
\end{equation}

La condición de balance es una condición \emph{necesaria} para que la distribución $p(\ss)$ sea una distribución invariante.
Una manera posible de satisfacer esta ecuación es igualando de manera individual los términos de los dos lados. De ahí resulta una condición
\emph{suficiente}, pero no necesaria, llamada la \defn{condición de balance detallado}:
\begin{equation}
p(\ss) P(\ss \to \tau) =  p(\tau) P(\tau \to \ss),
\label{eq:balance-detallado}
\end{equation}
que se tiene para \emph{todas} las configuraciones $\ss$ y $\tt$.
Podemos pensar que esta ecuación especifica que los flujos de probabilidad en equilibrio de una configuación a otra es igual al flujo de regreso.

Normalmente, queremos generar una distribución de equilibrio $p(\ss)$ dada. Así que la versión de la condición de balance detallado que nos interesa es:
\begin{equation}
\frac{P(\ss \to \tt)}{P(\tt \to \ss)} =  \frac{p(\tt)}{p(\ss)}.
\label{eq:balance-detallado-nuevo}
\end{equation}
En el caso del muestreo de Boltzmann, la distribución de equilibrio que nos interesa es justamente la de Boltzmann. Por lo tanto, obtenemos que
\begin{equation}
\frac{P(\ss \to \tt)}{P(\tt \to \ss)} =  \frac{p(\tt)}{p(\ss)} = \frac{\frac{1}{Z} e^{-\beta \HH(\tt)}}{\frac{1}{Z} e^{-\beta \HH(\ss)}} = e^{-\beta \Delta \HH},
\label{eq:balance-detallado-boltz}
\end{equation}
donde hemos definido \( \Delta \HH \defeq \HH(\tt) - \HH(\ss) \), es decir, $\Delta \HH$ es la diferencia de energías entre la configuración nueva y la anterior.
Ésta es la condición que se tiene que satisfacer, entonces, para generar la distribución de Boltzmann.  Nótese que los factores de la función de partición $Z$ \emph{se cancelan} de la ecuación. Es por eso que podemos generar la distribución adecuada, sin tener que conocer el valor de $Z$; ésta es la fuerza y la clave del método.


\subsubsection{Ergodicidad}
Otra condición importante que tiene que satisfacer el proceso es la de \defn{ergodicidad}. Eso requiere que cualquier configuración está accesible
desde cualquier otra, a través un número finito de pasos. Si no fuera el caso, entonces podría haber unas partes del espacio de configuraciones que nunca se
pudiera alcanzar.


\section{Regla de Metropolis}
Nos queda la tarea de resolver la ecuación~\eqref{eq:balance-detallado-boltz} para las probabilidades de transición $P(\ss \to \tt)$ para una distribución de equilbrio $p(\ss)$ dada. La manera que adoptaremos --que está lejos de ser única-- se debe a Metropolis et al., quien lo inventaron a principos de los 1950s.

Metropolis et al.~se dieron cuenta de que podemos reducir la dificultad de resolver la ecuación~\eqref{eq:balance-detallado-boltz} al separar la probabilidad de transición $P(\ss \to \tt)$ en dos partes:
\begin{equation}
 P(\ss \to \tt) = g(\ss \to \tt) \, \alpha(\ss \to \tt),
\end{equation}
donde $g(\ss \to \tt)$ es una probabilidad de \emph{generar}, o \emph{proponer} el cambio hacia la nueva configuración $\tt$, y $\alpha(\ss \to \tt)$ es la probabilidad de que \defn{aceptemos} este cambio si se propone.  Si no aceptamos el cambio, es decir que lo \defn{rechazamos}, entonces el sistema se queda en la misma configuración $\ss$. Por lo tanto, hay en general una probabilidad no-cero de quedarse en el mismo estado, lo que implica que $P(\ss \to \ss) \neq 0$.

La idea es que ahora nosotros imponemos las probabilidades $g(\ss \to \tt)$ de proponer cambios que queramos, por ejemplo los que sean fáciles de implementar.
Luego hace falta resolver para las $\alpha(\ss \to \tt)$. Esto resulta ser más fácil si las $g(\ss \to \tt)$ se escogen de manera que sean \emph{simétricas}, ya que en este caso
tenemos que
\begin{equation}
\frac{P(\ss \to \tt)}{P(\tt \to \ss)}  = \frac{g(\ss \to \tt) \alpha(\ss \to \tt)}{g(\tt \to \ss) \alpha(\tt \to \ss)}  = \frac{\alpha(\ss \to \tt)}{\alpha(\tt \to \ss)}.
\end{equation}

Como ejemplo, consideremos el caso del modelo de Ising. La manera más sencilla de generar nuevas configuraciones es que cambiemos solamente un espín a la vez.
Así que las probabilidades de generación $g(\ss \to \tt)$ son $1/N$ si las configuraciones $\ss$ y $\tt$ difieren en solamente un espín, mientras que son $0$ si no. En este caso, la probabilidad de regresar a la configuración anterior es igual, y entonces $g(\ss \to \tt)$ sí es simétrica.

\subsubsection{Solución de Metropolis et al.\ para probabilidades de aceptación}
Finalmente, nos quedamos con la ecuación
\begin{equation}
  \frac{\alpha(\ss \to \tt)}{\alpha(\tt \to \ss)} = e^{-\beta \Delta \HH}
\end{equation}
en el caso de la ecuación de Boltzmann. (En otro caso, el miembro de derecha de la ecuación será distinto, pero el siguiente argumento también se aplica.)

No queremos que las $\alpha(\ss \to \tt)$ sean pequeñas, ya que en este caso el sistema se quedará mucho tiempo en cada configuración, y así no explorará bien las configuraciones posibles. Por lo tanto, queremos \emph{maximizar} las $\alpha(\ss \to \tt)$.  Ya que son probabilidades, están acotadas entre $0$ y $1$. Por lo tanto, ponemos el más grande de $\alpha(\ss \to \tt)$ y $\alpha(\tt \to \ss)$ igual a $1$. Si $\Delta \HH > 0$, es decir que $\HH(\tt) > \HH(\ss)$, entonces $e^{-\beta \Delta \HH} < 1$, y entonces según la ecuación vemos que $\alpha(\ss \to \tt) < \alpha(\tt \to \ss)$. Así que ponemos $\alpha(\tt \to \ss) = 1$, lo que implica que $\alpha(\ss \to \tt) = e^{-\beta \Delta \HH}$.

Finalmente, encontramos la \defn{regla de Metropolis}: el cambio de la configuración $\ss$ a la configuración nueva $\tau$ se acepta con probabilidad
\begin{equation}
 \alpha(\ss \to \tt) = \begin{cases}
                        e^{-\beta \Delta \HH}, &\text{si } \Delta \HH > 0\\
		      1, &\text{si no}
                       \end{cases}
= \min\{1, e^{-\beta \Delta \HH} \}.
\end{equation}
Nótese que el cambio siempre se acepta si la energía de la configuración nueva es menor o igual que la energía de la configuración anterior.
Aunque eso sea sorprendente, resulta ser el método más eficiente, como acabamos de demostrar.

\section{Algoritmo de Metropolis}
Ya tenemos todos los ingredientes para simular el modelo de Ising --o, de hecho, cualquier modelo cuyo Hamiltoniano conocemos-- a una temperatura fija:
\begin{enumerate}

\item Empezar con una configuración inicial $\ss$.
\item Proponer una nueva configuración $\tau$ de manera simétrica --en el modelo de Ising, cambiar un espín.
\item Calcular el cambio de la energía total que resulta, $\Delta \HH \defeq \HH(\tt) - \HH(\ss)$.
\item Si $\Delta \HH \le 0$, entonces aceptar el cambio --el sistema se queda en el nuevo estado $\tt$.\\
  Si $\Delta \HH > 0$, entonces aceptar el cambio con la probabilidad $e^{-\beta \Delta \HH}$. Si no, entonces rechazar el cambio --el sistema se queda en el estado $\ss$.
\item Repetir desde (2).
\end{enumerate}

Lo que hemos mostrado en este capítulo es que el algoritmo anterior genera con éxito, en el límite de tiempo largo, las configuraciones $\ss$ con frecuencias que convergen a la distribución de Boltzmann $p(\ss)$.






